# ğŸ¹ Silentâ€‘Video Piano Key Detection

> **Final short project â€“ Image Analysis & Computer Vision (SpringÂ 2025)**

---

## ğŸ“– Overview

This repository contains the PythonÂ 3.10 code, sample data, and report for my university project.
Given **only** a silent topâ€‘down (or slightly angled) recording of a pianist, the program:

1. *Rectifies* the keyboard with homography to undo perspective distortion.
2. Uses **MediaPipe Hands** to track finger contact candidates.
3. Decides which **white & black keys** are actually depressed frameâ€‘byâ€‘frame.
4. **Postâ€‘processes / "softens"** the detections to remove flicker.
5. Exports the result to a standardsâ€‘compliant **MIDI** file you can instantly play back in tools like [Pianotify](https://pianotify.com/import-midi-file).

The full classical (nonâ€‘DL) pipeline, including many pitfalls and future work, is detailed in `project_report.pdf`.

---

## ğŸ—‚ï¸ Folder Structure

```text
project_root/
â”œâ”€â”€ main.py                 # Entryâ€‘point script (run everything from here)
â”œâ”€â”€ project_report.pdf      # Written report (methodology & discussion)
â”œâ”€â”€ requirements.txt        # Precise Python dependencies
â”œâ”€â”€ inputs/
â”‚   â””â”€â”€ <video_name>/       # One subâ€‘folder per test video
â”‚       â”œâ”€â”€ params.json     # Fineâ€‘tuning thresholds for this take
â”‚       â”œâ”€â”€ reference.png   # Clean keyboard image (no hands)
â”‚       â””â”€â”€ video.mp4       # The raw performance
â””â”€â”€ outputs/
    â””â”€â”€ <video_name>/       # Autoâ€‘generated results live here
        â”œâ”€â”€ output.json     # Raw perâ€‘frame key states
        â”œâ”€â”€ softened.json   # Temporal smoothing of `output.json`
        â”œâ”€â”€ output.mid      # The extracted performance
        â”œâ”€â”€ frames/         # Debug visuals (hands + key overlay)
        â”œâ”€â”€ homography/     # Debug: rectification steps
        â”œâ”€â”€ keys_touched/   # Debug: fingerâ€‘key proximity
        â””â”€â”€ is_toggled/     # Debug: final "pressed?" decision
```

*Tip:* Supply your own `inputs/<video_name>` folder with the three required files to test a new piece.

---

## âš™ï¸ Installation

> **Prerequisite:** PythonÂ 3.10

1. **Create** and activate a virtual environment *(recommended)*:

   ```bash
   python -m venv .venv
   source .venv/bin/activate   # macOSÂ / Linux
   .venv\Scripts\activate     # Windows PowerShell
   ```
2. **Install** dependencies:

   ```bash
   pip install -r requirements.txt
   ```

   | Package         | Version   | OSâ€‘specific note                                                                                                                                                  |
   | --------------- | --------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
   | `opencv-python` | 4.11.0.86 | On some Linux distros you may need extra system libs (e.g., `apt install libgl1`).  If you require nonâ€‘free codecs or SIFT, grab `opencv-contrib-python` instead. |
   | `mediapipe`     | 0.10.21   | Requires a modern pip (â‰¥Â 23) to pull prebuilt wheels.                                                                                                             |

If you hit OpenCV build errors on AppleÂ Silicon, try:

```bash
pip install opencv-python-headless
```

then **brew install libjpeg libpng openexr** and reâ€‘run.

---

## ğŸš€ Usage

Run the pipeline from the project root:

```bash
python main.py -i <video_name> [--calibrate] [--show]
```

| Flag          | Short | Required | Description                                                                                      |
| ------------- | ----- | -------- | ------------------------------------------------------------------------------------------------ |
| `--input`     | `-i`  | **Yes**  | Name of the subâ€‘folder inside `inputs/` (and destination under `outputs/`).                      |
| `--calibrate` | `-c`  | No       | *Reâ€‘process* the video endâ€‘toâ€‘end.  Omit to reâ€‘use existing `output.json` to shorten dev cycles. |
| `--show`      | `-s`  | No       | Write all debug images & visualisations.  Skipping this makes processing \~3Ã— faster.            |

### Example

```bash
python main.py -i midna -c -s
```

1. Processes `inputs/midna/video.mp4`
2. Saves results in `outputs/midna/`
3. Opens `output.mid` in [Pianotify](https://pianotify.com/import-midi-file) to check accuracy.

---

## âœ¨ Expected Output

After a successful run with `--show`, youâ€™ll get something like:

* **output.mid** â†’ audible reconstruction of the piece.
* **frames/** â†’ GIFâ€‘like sequence with *red* possible keys & *green* confirmed presses.
* **homography/**, **keys\_touched/**, **is\_toggled/** â†’ stepâ€‘byâ€‘step visuals for academic writeâ€‘ups.

*(For a deep dive on evaluation metrics, and blackâ€‘key woes, see `project_report.pdf`.)*

---

## ğŸ™ Acknowledgements

* **Professor Vicenzo Caglioti** for guidance on the project solution. 
* **MediaPipe** for realâ€‘time hand tracking.
* **OpenCV** & **NumPy** for the heavy lifting.
